# MLPerf Inference v2.1 NVIDIA-Optimized Implementations of Triton Inference Server running on CPU
This is a repository of NVIDIA-optimized implementations for the [MLPerf](https://mlcommons.org/en/) Inference Benchmark.
This README is a quickstart tutorial on how to use our code for Triton on CPU systems as a public / external user.
It is recommended to also read README.md for general instructions

**NOTE**: This document is autogenerated from internal documentation. If something is wrong or confusing, please contact NVIDIA.

---

NVIDIA's Triton Inference Server is an open-source inference serving software for deploying trained AI models at scale in production, optimized for performance and designed to provide a consistent interface regardless of whether execution is performed on GPUs or other hardware. In NVIDIA's MLPerf Inference v2.0 submission, Triton's OpenVINO CPU backend is improved by additionally supporting the OpenVINO libraries built from origin/master source using a more recent commit.

1. OpenVINO 2021.4 with default Intel Thread Building Blocks (TBB) threading support, which demonstrates better offline throughput and competitive server performance.
2. OpenVINO 2021.2 built from source with OpenMP threading enabled, which demonstrates better server performance on BERT.
3. OpenVINO origin/master commit f2f281e60b20e6489e5eaa90e52fbd15debb4f0a

This submission allows the user to switch between different versions depending on the benchmark, taking advantage of the benefits offered by each.

### NVIDIA Submissions

The Triton CPU submission supports:

- BERT (Offline, Server), at 99% of FP32 accuracy target
- ResNet50 (Offline, Server), at 99% of FP32 accuracy target
- SSD-ResNet34 (Offline, Server), at 99% of FP32 accuracy target

See the main README.md for an explanation of the benchmarks and scenarios.

All models are quantized to INT8. Directions for generating the INT8 models can be found in the `calibration_triton_cpu/` directory. Models generated using the instructions described must then be copied into `MLPERF_CPU_SCRATCH_PATH` as follows:

- `<MLPERF_CPU_SCRATCH_PATH>/models/Triton/bert_int8_openvino/1/model.xml`
- `<MLPERF_CPU_SCRATCH_PATH>/models/Triton/bert_int8_openvino/1/model.bin`
- `<MLPERF_CPU_SCRATCH_PATH>/models/Triton/resnet50_int8_openvino/1/model.xml`
- `<MLPERF_CPU_SCRATCH_PATH>/models/Triton/resnet50_int8_openvino/1/model.bin`
- `<MLPERF_CPU_SCRATCH_PATH>/models/Triton/ssd-resnet34_int8_openvino/1/model.xml`
- `<MLPERF_CPU_SCRATCH_PATH>/models/Triton/ssd-resnet34_int8_openvino/1/model.bin`
- `<MLPERF_CPU_SCRATCH_PATH>/preprocessed_data/[datasets]`

To generate the preprocessed datasets, follow the benchmark-specific instructions described in the `[README.md](http://README.md)` files stored in `code/[benchmark]/openvino` for each benchmark.

### NVIDIA Submission Systems

The Intel Xeon CPU systems that NVIDIA support, has tested, and is submitting to MLPerf Inference v2.0 are:

- Dual Socket Xeon Platinum 8380 Ice Lake
- Quad Socket Xeon Platinum 8380H Cooper Lake

### Setup

Like the GPU-based submission, `closed/NVIDIA` should be the working directory when running any commands. You should also **not execute any commands** with the user `root`. Doing so may cause permission errors.

We recommend using Ubuntu 18.04. Other operating systems have not been tested.

You will first need to set up your scratch space - We recommend that the storage space have at least 1TB of free space to comfortably store models, datasets, and preprocessed datasets.

To set your scratch path, run:

```
export MLPERF_CPU_SCRATCH_PATH=path/to/scratch/space
```
To use the CPU code paths, set the `USE_CPU` environment variable in order to properly build and set up the container. Note that all CPU Triton related dependencies, including backend libraries, model files, and datasets should be aggregated into one directory mapped to `MLPERF_CPU_SCRATCH_PATH`. The models can be built by following the directions in `closed/NVIDIA/calibration_triton_cpu`.

```
export USE_CPU=1
make prebuild
```
The docker image will have the tag `mlperf-inference:[username]-latest`. The working directory (`closed/NVIDIA`) will also be mounted in `/work` inside the container. Once in the container, build the harness binaries with

```
make build_cpu
```
Before running benchmarks, it is recommended to run the following commands to put the system into a performance-oriented state, such as setting Transparent Huge Pages (THP) to **always**. Note that these commands may need to be run **outside of the MLPerf docker container** (i.e. in a separate console prior to running the benchmark, explained in the next section).

```
echo 0 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
echo 0 | sudo tee /proc/sys/kernel/numa_balancing
echo 100 | sudo tee /sys/devices/system/cpu/intel_pstate/min_perf_pct
echo never  | sudo tee /sys/kernel/mm/transparent_hugepage/enabled; sleep 1
echo never  | sudo tee  /sys/kernel/mm/transparent_hugepage/defrag; sleep 1
echo always | sudo tee /sys/kernel/mm/transparent_hugepage/enabled; sleep 1
echo always | sudo tee /sys/kernel/mm/transparent_hugepage/defrag; sleep 1
echo 1 | sudo tee /proc/sys/vm/compact_memory
echo 3 | sudo tee /proc/sys/vm/drop_caches
```
### Running a Benchmark

To run a benchmark, use `make run_cpu_harness`. Unlike the normal GPU-based NVIDIA submission, there is **no 'generate engines' step**, as that is a phase specific to TensorRT. In general, you can follow the instructions for the GPU-based submission in the main README.md, except use `run_cpu_harness` as the make target, and set `--config_ver=openvino in RUN_ARGS`.

As an example, to run ResNet50 in Offline scenario with the openvino backend, run:

```
make run_cpu_harness RUN_ARGS="--benchmarks=resnet50 --scenarios=offline --config_ver=openvino --verbose" VERBOSE=1
```
To run in Accuracy mode, add `--test_mode=AccuracyOnly` to `RUN_ARGS`, just like with the GPU-based submission:

```
make run_cpu_harness RUN_ARGS="--benchmarks=resnet50 --scenarios=offline --config_ver=openvino --verbose --test_mode=AccuracyOnly" VERBOSE=1
```
Replace `--benchmarks` and `--scenarios` as normal, just like with the GPU-based submission.

By default, the harness will use OpenVINO library built from commit hash f2f281e60b20e6489e5eaa90e52fbd15debb4f0a. This can be specified explicitly using the RUN_ARG switch:

```
make run_cpu_harness RUN_ARGS="--benchmarks=resnet50 --scenarios=offline --config_ver=openvino --openvino_version=f2f281e6" VERBOSE=1
```
To use other versions of OpenVINO libraries, one of the following can be used:

```
make run_cpu_harness RUN_ARGS="--benchmarks=resnet50 --scenarios=offline --config_ver=openvino --openvino_version=2021_2" VERBOSE=1
make run_cpu_harness RUN_ARGS="--benchmarks=resnet50 --scenarios=offline --config_ver=openvino --openvino_version=2021_4_pre" VERBOSE=1
```
We recommend using the default for bert and `2021_4_pre` for resnet50 and ssd-resnet34, which is what is used for this submission.

Follow the steps in the main README.md for instructions on compliance tests and making an actual submission.

